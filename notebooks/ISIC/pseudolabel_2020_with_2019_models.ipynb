{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd \n",
    "import torch\n",
    "\n",
    "from importlib import import_module\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms.functional import hflip, vflip \n",
    "from tqdm import tqdm \n",
    "\n",
    "from skp.toolbox.functions import load_model_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelanomaDataset(Dataset):\n",
    "\n",
    "    def __init__(self, files, image_sizes):\n",
    "        self.files = files\n",
    "        self.image_sizes = image_sizes\n",
    "        self.transforms = {image_size: v2.Compose([v2.ToImage(), v2.Resize(image_size)]) for image_size in image_sizes}\n",
    "        self.center_crop = {image_size: v2.CenterCrop((image_size, image_size)) for image_size in image_sizes}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def preprocess(self, img, image_size):\n",
    "        img = self.transforms[image_size](img.copy())\n",
    "        h, w = img.shape[1:]\n",
    "        if h > w:\n",
    "            crop0 = img[:, :image_size]\n",
    "            crop1 = img[:, -image_size:]\n",
    "        elif h < w:\n",
    "            crop0 = img[:, :, :image_size]\n",
    "            crop1 = img[:, :, -image_size:]\n",
    "        elif h == w:\n",
    "            crop0 = hflip(img)\n",
    "            crop1 = vflip(img)\n",
    "        crop2 = self.center_crop[image_size](img)\n",
    "        return torch.stack([crop0, crop1, crop2]).float()\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        img = cv2.imread(self.files[i], cv2.IMREAD_COLOR)\n",
    "        img_dict = {image_size: self.preprocess(img, image_size) for image_size in self.image_sizes}\n",
    "        return img_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_name = \"melanoma.cfg_2019_baseline\"\n",
    "cfg_512 = import_module(f\"skp.configs.{cfg_name}\").cfg\n",
    "cfg_name = \"melanoma.cfg_2019_baseline_640\"\n",
    "cfg_640 = import_module(f\"skp.configs.{cfg_name}\").cfg\n",
    "cfg_name = \"melanoma.cfg_2019_baseline_768\"\n",
    "cfg_768 = import_module(f\"skp.configs.{cfg_name}\").cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "model_dict[512] = load_model_from_config(cfg_512, weights_path=\"/home/ian/projects/SKP/experiments/melanoma/melanoma.cfg_2019_baseline/e1638ae8/fold0/checkpoints/last.ckpt\", device=\"cuda\", eval_mode=True)\n",
    "model_dict[640] = load_model_from_config(cfg_640, weights_path=\"/home/ian/projects/SKP/experiments/melanoma/melanoma.cfg_2019_baseline_640/8bfd2448/fold1/checkpoints/last.ckpt\", device=\"cuda\", eval_mode=True)\n",
    "model_dict[768] = load_model_from_config(cfg_768, weights_path=\"/home/ian/projects/SKP/experiments/melanoma/melanoma.cfg_2019_baseline_768/c9739c02/fold2/checkpoints/last.ckpt\", device=\"cuda\", eval_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/mnt/stor/datasets/ISIC/2020/ISIC_2020_Training_GroundTruth_v2.csv\")\n",
    "files = df[\"image_name\"].tolist()\n",
    "files = [f\"/mnt/stor/datasets/ISIC/2020/train/{f}.jpg\" for f in files]\n",
    "files[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MelanomaDataset(files, image_sizes=[512, 640, 768])\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "pred_list = []\n",
    "for batch in tqdm(dataloader):\n",
    "    y_list = []\n",
    "    for k, v in batch.items():\n",
    "        x = v[0].cuda()\n",
    "        with torch.inference_mode():\n",
    "            y = model_dict[k]({\"x\": x})\n",
    "            y = y[\"logits\"].softmax(dim=1).mean(0)\n",
    "        y_list.append(y)\n",
    "    y = torch.stack(y_list).mean(0)\n",
    "    pred_list.append(y.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(np.stack(pred_list))\n",
    "pred_df.columns = [\"MEL\", \"NV\", \"BCC\", \"AK\", \"BKL\", \"DF\", \"VASC\", \"SCC\", \"UNK\"]\n",
    "pred_df[\"filepath\"] = files[:len(pred_df)]\n",
    "pred_df[\"filepath\"] = pred_df.filepath.apply(lambda x: os.path.basename(x))\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(\"/mnt/stor/datasets/ISIC/2020/train_pseudolabels_from_2019_models.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = pd.read_csv(\"/mnt/stor/datasets/ISIC/2020/ISIC_2020_Training_GroundTruth.csv\")\n",
    "pred_df[\"image_name\"] = pred_df.filepath.apply(lambda x: x.split(\".\")[0])\n",
    "gt_df = gt_df.merge(pred_df, on=\"image_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "roc_auc_score(gt_df.target.values, gt_df.MEL.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for k, v in batch.items():\n",
    "    imgs = v[0]\n",
    "    for img in imgs:\n",
    "        plt.imshow(img.permute(1, 2, 0).long().numpy())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
