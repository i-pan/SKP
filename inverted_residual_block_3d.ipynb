{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_groups(group_size: int | None, channels: int):\n",
    "    if not group_size:  # 0 or None\n",
    "        return 1  # normal conv with 1 group\n",
    "    else:\n",
    "        # NOTE group_size == 1 -> depthwise conv\n",
    "        assert channels % group_size == 0\n",
    "        return channels // group_size\n",
    "\n",
    "\n",
    "def make_divisible(v: int, divisor: int = 8, min_value: int | None = None, round_limit: float = .9):\n",
    "    min_value = min_value or divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < round_limit * v:\n",
    "        new_v += divisor\n",
    "    return new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcite(nn.Module):\n",
    "    \"\"\" Squeeze-and-Excitation w/ specific features for EfficientNet/MobileNet family\n",
    "\n",
    "    Args:\n",
    "        in_chs (int): input channels to layer\n",
    "        rd_ratio (float): ratio of squeeze reduction\n",
    "        act_layer (nn.Module): activation layer of containing block\n",
    "        gate_layer (Callable): attention gate function\n",
    "        force_act_layer (nn.Module): override block's activation fn if this is set/bound\n",
    "        rd_round_fn (Callable): specify a fn to calculate rounding of reduced chs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_chs: int,\n",
    "            rd_ratio: float = 0.25,\n",
    "            rd_channels: int | None = None,\n",
    "            act_layer: Callable = nn.ReLU,\n",
    "            gate_layer: Callable = nn.Sigmoid,\n",
    "            force_act_layer: Callable | None = None,\n",
    "            rd_round_fn: Callable | None = None,\n",
    "    ):\n",
    "        super(SqueezeExcite, self).__init__()\n",
    "        if rd_channels is None:\n",
    "            rd_round_fn = rd_round_fn or round\n",
    "            rd_channels = rd_round_fn(in_chs * rd_ratio)\n",
    "        act_layer = force_act_layer or act_layer\n",
    "        self.conv_reduce = nn.Conv3d(in_chs, rd_channels, 1, bias=True)\n",
    "        self.act1 = act_layer()\n",
    "        self.conv_expand = nn.Conv3d(rd_channels, in_chs, 1, bias=True)\n",
    "        self.gate = gate_layer()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_se = x.mean((2, 3, 4), keepdim=True)\n",
    "        x_se = self.conv_reduce(x_se)\n",
    "        x_se = self.act1(x_se)\n",
    "        x_se = self.conv_expand(x_se)\n",
    "        return x * self.gate(x_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerScale3d(nn.Module):\n",
    "    def __init__(self, dim: int, init_values: float = 1e-5, inplace: bool = False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        gamma = self.gamma.view(1, -1, 1, 1, 1)\n",
    "        return x.mul_(gamma) if self.inplace else x * gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalInvertedResidual3d(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chs: int, \n",
    "        out_chs: int, \n",
    "        dw_kernel_size_start: int = 0,\n",
    "        dw_kernel_size_mid: int = 3, \n",
    "        dw_kernel_size_end: int = 0,\n",
    "        stride: int | tuple[int, int, int] = (2, 1, 1), \n",
    "        dilation: int = 1, \n",
    "        padding: int = 1, \n",
    "        group_size: int = 1,  \n",
    "        noskip: bool = False, \n",
    "        exp_ratio: float = 1.0,\n",
    "        act_layer: Callable = nn.ReLU, \n",
    "        norm_layer: Callable = nn.BatchNorm3d, \n",
    "        se_layer: bool = False,\n",
    "        conv_kwargs: dict | None = None, \n",
    "        layer_scale_init_value: float | None = 1e-5,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        conv_kwargs = conv_kwargs or {}\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride, stride)\n",
    "        self.has_skip = (in_chs == out_chs and stride[0] in {1, 2}) and not noskip\n",
    "        if stride > 1: \n",
    "            assert dw_kernel_size_start or dw_kernel_size_mid or dw_kernel_size_end\n",
    "        \n",
    "        if dw_kernel_size_start:\n",
    "            dw_start_stride = stride if not dw_kernel_size_mid else 1 \n",
    "            dw_start_groups = num_groups(group_size, in_chs)\n",
    "            self.dw_start = nn.Sequential(\n",
    "                nn.Conv3d(in_chs, in_chs, kernel_size=dw_kernel_size_start, stride=dw_start_stride, dilation=dilation, padding=padding, groups=dw_start_groups, bias=False),\n",
    "                norm_layer(in_chs),\n",
    "                # no activation\n",
    "            )\n",
    "        else:\n",
    "            self.dw_start = nn.Identity()\n",
    "        \n",
    "        mid_chs = make_divisible(in_chs * exp_ratio)\n",
    "        self.pw_exp = nn.Sequential(\n",
    "            nn.Conv3d(in_chs, mid_chs, 1, bias=False),\n",
    "            norm_layer(mid_chs),\n",
    "            act_layer(),\n",
    "        )\n",
    "\n",
    "        if dw_kernel_size_mid:\n",
    "            groups = num_groups(group_size, mid_chs)\n",
    "            self.dw_mid = nn.Sequential(\n",
    "                nn.Conv3d(mid_chs, mid_chs, dw_kernel_size_mid, stride=stride, dilation=dilation, padding=padding, groups=groups, bias=False),\n",
    "                norm_layer(mid_chs),\n",
    "                act_layer(),\n",
    "            )\n",
    "        \n",
    "        self.se = SqueezeExcite(mid_chs, act_layer=act_layer) if se_layer else nn.Identity()\n",
    "\n",
    "        self.pw_proj = nn.Sequential(\n",
    "            nn.Conv3d(mid_chs, out_chs, 1, bias=False),\n",
    "            norm_layer(out_chs),\n",
    "            # no activation\n",
    "        )\n",
    "\n",
    "        if dw_kernel_size_end:\n",
    "            dw_end_stride = stride if not dw_kernel_size_start and not dw_kernel_size_mid else 1\n",
    "            dw_end_groups = num_groups(group_size, out_chs)\n",
    "            self.dw_end = nn.Sequential(\n",
    "                nn.Conv3d(out_chs, out_chs, dw_kernel_size_end, stride=dw_end_stride, dilation=dilation, padding=padding, groups=dw_end_groups, bias=False),\n",
    "                norm_layer(out_chs),\n",
    "                act_layer(),\n",
    "            )\n",
    "        else:\n",
    "            self.dw_end = nn.Identity()\n",
    "        \n",
    "        if layer_scale_init_value is not None:\n",
    "            self.layer_scale = LayerScale3d(out_chs, layer_scale_init_value)\n",
    "        else:\n",
    "            self.layer_scale = nn.Identity()\n",
    "        \n",
    "        if stride == 2 and self.has_skip:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.AvgPool3d(stride, stride), \n",
    "                nn.Conv3d(in_chs, out_chs, 1, padding=0, bias=False),\n",
    "                norm_layer(out_chs),\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.dw_start(x)\n",
    "        x = self.pw_exp(x)\n",
    "        x = self.dw_mid(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pw_proj(x)\n",
    "        x = self.dw_end(x)\n",
    "        x = self.layer_scale(x)\n",
    "        if self.has_skip:\n",
    "            x = x + self.downsample(shortcut)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniversalInvertedResidual3d(\n",
       "  (dw_start): Sequential(\n",
       "    (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64, bias=False)\n",
       "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (pw_exp): Sequential(\n",
       "    (0): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "  )\n",
       "  (dw_mid): Sequential(\n",
       "    (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), groups=256, bias=False)\n",
       "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "  )\n",
       "  (se): SqueezeExcite(\n",
       "    (conv_reduce): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (act1): GELU(approximate='none')\n",
       "    (conv_expand): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (gate): Sigmoid()\n",
       "  )\n",
       "  (pw_proj): Sequential(\n",
       "    (0): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (dw_end): Sequential(\n",
       "    (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64, bias=False)\n",
       "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "  )\n",
       "  (layer_scale): LayerScale3d()\n",
       "  (downsample): Sequential(\n",
       "    (0): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
       "    (1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    (2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = UniversalInvertedResidual3d(64, 64, 3, 3, 3, 2, 1, 1, exp_ratio=4.0, act_layer=nn.GELU, norm_layer=nn.BatchNorm3d, se_layer=True)\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 32, 32, 32])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((2, 64, 64, 64, 64))\n",
    "block(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
